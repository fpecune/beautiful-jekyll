---
layout: page
title: Research Projects
css: css/style.css
---

<!DOCTYPE html>
<html>
    <head>
        <meta charset="utf-8" />
		<link rel="stylesheet" href="style.css" />
        <title>Florian Pecune</title>
    </head>
 
    <body bgcolor="#E6E6FA">
	<div class="wrapper">
	<header>
	 <img id="personalpic" src="images/personalphoto.png" alt="personal photo" />
	 <img id="logocmu" src="images/logo-cmu.jpg" alt="logo telecom paristech" />
	<h1>Florian Pecune</h1>
	<p class="title">Ph.D Student in Computer Science</p>
	</header>
	
	
	<div class="publiType">
	Decision-making model of artificial companions in different social contexts
	</div>
	
	<div class="researchlefttext">
<strong>Phd Thesis<br>
Supervised by <a title="Catherine" href="http://perso.telecom-paristech.fr/~pelachau/">Catherine Pelachaud</a> and <a title="Magalie" href="http://magalie.ochs.free.fr/index.html">Magalie Ochs</a><br>
Collaboration with <a title="Research" href="http://www.northeastern.edu/cesar/?page_id=20">CESAR Team</a>, Northeastern University, USA</strong></br></br>
	For my Ph.D thesis, I work on building a virtual agent able to adapt its social attitude toward a user during an interaction. The agent's attitude varies according to its
	social relation toward the user, and its social role during the interaction. <br><br> 

	I implemented my model using PsychSim: the agent's sequence of actions is generated according to its affective state, its goals, and its beliefs about the user.
	I am currently working on an evaluation protocol to check whether the agent's social attitude can be recognized through this sequence of actions.
	

	</div>
		
	<img id="researchrightimage" src="images/phdarchi.png" alt="SOCRATES architecture" />
	<br><br>
	<div class="researchpubli">
		<b><font color=#800000> Publication :</font></b><br><br>
		<strong> SOCRATES: from SOCial Relations to ATtitude ExpressionS </strong></br>
		Florian Pecune, Magalie Ochs, Stacy Marsella, Catherine Pelachaud </br>
		<i>International Conference on Autonomous Agents and Multiagent Systems (AAMASâ€™16) </i></br>
		Singapore, May 2016. [<a href="pdf/AAMAS2016.pdf" title="Download in pdf format">pdf</a>] </br>
	</div>
	
	
	
	<div class="publiType">
	Motor resonance and mimicry during dyadic interactions
	</div>
	
	<div class="researchlefttext">
<strong>Collaboration with <a title="Research" href="https://ive.scm.tees.ac.uk/?pID=1">Intelligent Virtual Environments Team</a>, Teesside University, England</strong></br></br>
	In this project, we used a Brain-Computer Interface (BCI) in a neurofeedback setting. The purpose of this experiment was to investigate the potential of using a single high-level
emotional dimension as an input to control a virtual agent's response. <br><br>

My work was to design a virtual agent able to align in real-time its nonverbal behavior to the user's detected level of empathy: as user's positive thoughts increased, the
agent gave more positive feedbacks as sign of its engagement. To do so, I mapped the user's level of empathy into animation parameters for the agent (AUs and BAPs).<br><br>

	</div>
		
	<img id="researchrightimage" src="images/ICMIExpe.png" alt="ECA Control using a Single Affective User Dimension" />
	
	<div class="researchpubli">
		<b><font color=#800000> Publication :</font></b><br><br>
		<strong> ECA Control using a Single Affective User Dimension </strong></br>
		Fred Charles, Florian Pecune, Gabor Aranyi, Catherine Pelachaud, Marc Cavazza</br>
		<i>International Conference on Multimodal Interaction (ICMI'15) </i></br>
		Seattle, USA, November 2015. [<a href="pdf/ICMI2015.pdf" title="Download in PDF format">PDF</a>] </br>
	</div>
	
		<div class="researchlefttext">
<strong>Collaboration with <a title="Research" href="http://www.infomus.org/index_eng.php">Infomus Team</a>, Genova University, Italy</strong></br></br>
	In this project, we aimed at building a laughing virtual agent. We focused on:
(1) how the user's perception of the stimuli funniness was affected
by the presence of a laughing agent, performing prescripted
Vs. copying behavior; (2) how the user's mood
varied according to the different conditions.  <br><br>

My work was to design a virtual agent able to adapt in real-time its laughing behavior to the user's laughter. To do so, I blended the user's detected parameters (body leaning and laughter intensity) with 
the animation parameters generated by our laughter animation model. 


	</div>
	<img id="researchrightimage" src="images/Ilhaire.jpg" alt="Laughing with a virtual agent" />
	
	<div class="researchpubli">
	<b><font color=#800000> Publication :</font></b><br><br>
	<strong> Laughing with a Virtual Agent </strong></br>
	Florian Pecune, Beatrice Biancardi, Yu Ding, Catherine Pelachaud, Maurizio Mancini, Giovana Varni, Gualtiero Volpe, Antonio Camurri</br>
	<i>International Conference on Autonomous Agents and Multiagent Systems (AAMAS'15) </i></br>
	Istanbul, Turkey, May 2015. [<a href="pdf/AAMAS2015.pdf" title="Download in PDF format">PDF</a>] </br>
	</div>
	
	
	
	
	

</video>
	
	 <footer>

	 </footer>
	 </div>
    </body>
</html>
